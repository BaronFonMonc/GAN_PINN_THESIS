{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KT0W-1mvuHV"
      },
      "outputs": [],
      "source": [
        "DIR = 'gdrive/MyDrive/Colab_Notebooks/Images/GAN_with_PINN_rules_v.3/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYK_N60-7Z2H"
      },
      "source": [
        "# GAN с PINN, небольшой граф кровеноссной системы. \n",
        "* Изображения 3000х300. \n",
        "* Здоровые настоящие, Больные сгенерированные.\n",
        "* Два выхода сети\n",
        "* Функция активации LeakyRelu\n",
        "* Третья версия интервалов. Добавляем минимальное расстояние до ближайшей границы, но теперь это отдельная функция потерь. Bias.\n",
        "* Невязки считаются как a/b-1 а не a-b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3PYlaRtrrQx"
      },
      "source": [
        "# Импорты\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J7gvi0tw-XF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import (Dense, \n",
        "                                     BatchNormalization, \n",
        "                                     LeakyReLU, \n",
        "                                     Reshape, \n",
        "                                     Conv2DTranspose,\n",
        "                                     Conv2D,\n",
        "                                     Dropout,\n",
        "                                     Flatten)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, accuracy_score, precision_score, recall_score, precision_recall_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLjaelc6jwm9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txRE3P16rsGR"
      },
      "source": [
        "## Качаем датасет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXJXkx4zTlEs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZFt2JGIUjZT"
      },
      "outputs": [],
      "source": [
        "data = np.load(\"gdrive/MyDrive/Colab_Notebooks/Images/small_data.npy\")\n",
        "labels = np.genfromtxt('gdrive/MyDrive/Colab_Notebooks/Images/all_target.csv', delimiter=',')\n",
        "labels = labels[1:].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh_GgorPVAo1"
      },
      "outputs": [],
      "source": [
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBMbbPBkDft9"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  plt.imshow(data[0][i])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htojaw_Wrsfo"
      },
      "source": [
        "## Готовим данные\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie7XjNlnOql0"
      },
      "outputs": [],
      "source": [
        "labels = np.array([1 if i < 3 else 0 for i in labels]) #1 - healthy | 0 - problems\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3BF-twBfhB-"
      },
      "outputs": [],
      "source": [
        "dataSingle = data.reshape(data.shape[0], 5120, 512).astype('float32') # Объединяем 10 картинок в одну высокую\n",
        "for i in range(data.shape[0]):\n",
        "  dataSingle[i] = dataSingle[i]/dataSingle[i].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNt2mgsDXu_0"
      },
      "outputs": [],
      "source": [
        "data = data.astype('float32')\n",
        "for i in range(data.shape[0]):\n",
        "  for j in range(10):\n",
        "    data[i][j] = data[i][j]/data[i][j].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIX5t0o85Jow"
      },
      "outputs": [],
      "source": [
        "test_dataSingle = dataSingle[:21]\n",
        "test_data = data[:21]\n",
        "test_labels = labels[:21] \n",
        "test_labels = test_labels.reshape(21,1)\n",
        "\n",
        "data = data[21:]\n",
        "dataSingle = dataSingle[21:]\n",
        "labels = labels[21:]\n",
        "labels = labels.reshape(60,1)\n",
        "\n",
        "print(test_dataSingle.shape)\n",
        "print(test_data.shape)\n",
        "print(test_labels.shape)\n",
        "print(dataSingle.shape)\n",
        "print(data.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIT8yACJ_7dn"
      },
      "source": [
        "# Генерируем фотографии (3000x300) с помощью PINN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rr9J5cHAIFh"
      },
      "source": [
        "## Меняем датасет\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjRgfJRStvY7"
      },
      "outputs": [],
      "source": [
        "data = np.load(\"gdrive/MyDrive/Colab_Notebooks/Images/small_data.npy\")\n",
        "labels = np.genfromtxt('gdrive/MyDrive/Colab_Notebooks/Images/all_target.csv', delimiter=',')\n",
        "labels = labels[1:].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZSwrDaVtvY8"
      },
      "outputs": [],
      "source": [
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK05bU9DuqXr"
      },
      "outputs": [],
      "source": [
        "dataPINN = [0]*81\n",
        "for i in range(81):\n",
        "  dataPINN[i] = [0]*10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4euWUiOWuK3Q"
      },
      "outputs": [],
      "source": [
        "for i in range(data.shape[0]):\n",
        "  for j in range(data.shape[1]):\n",
        "    dataPINN[i][j] = data[i][j][130:430, 100:400]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzL-4O_KtvY_"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  plt.imshow(dataPINN[0][i])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6f43qZQtvZC"
      },
      "outputs": [],
      "source": [
        "labels = np.array([1 if i < 3 else 0 for i in labels]) #1 - healthy | 0 - problems\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiPl6gP4vk7I"
      },
      "outputs": [],
      "source": [
        "dataPINN = np.asarray(dataPINN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR_ttue6tvZD"
      },
      "outputs": [],
      "source": [
        "dataPINN = dataPINN.reshape(dataPINN.shape[0], 3000, 300).astype('float32') # Объединяем 10 картинок в одну высокую\n",
        "for i in range(dataPINN.shape[0]):\n",
        "  dataPINN[i] = dataPINN[i]/dataPINN[i].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oni77A4nvwb0"
      },
      "outputs": [],
      "source": [
        "print(dataPINN.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh4_WAahtvZA"
      },
      "outputs": [],
      "source": [
        "plt.imshow(dataPINN[0].reshape(10,300,300)[0]) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkhxNTO8w5Ke"
      },
      "outputs": [],
      "source": [
        "test_dataPINN = dataPINN[:21]\n",
        "test_labels = labels[:21] \n",
        "test_labels = test_labels.reshape(21,1)\n",
        "\n",
        "dataPINN = dataPINN[21:]\n",
        "labels = labels[21:]\n",
        "labels = labels.reshape(60,1)\n",
        "\n",
        "print(test_dataPINN.shape)\n",
        "print(test_labels.shape)\n",
        "print(dataPINN.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dAeltkAw5Kh"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 3\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "my_labels = tf.data.Dataset.from_tensor_slices([i for i in range(60)])\n",
        "my_labels = my_labels.shuffle(1000, seed = RANDOM_SEED)\n",
        "my_labels = my_labels.batch(BATCH_SIZE)\n",
        "\n",
        "for i in my_labels:\n",
        "  print(i.tolist(), end = '\\t[')\n",
        "  print(labels[i.tolist()[0]], end = ', ')\n",
        "  print(labels[i.tolist()[1]], end = ', ')\n",
        "  print(labels[i.tolist()[2]], end = '] ')\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w72zy8Y1w5Kk"
      },
      "outputs": [],
      "source": [
        "my_labels = tf.data.Dataset.from_tensor_slices([i for i in range(60)])\n",
        "my_labels = my_labels.shuffle(1000, seed = RANDOM_SEED)\n",
        "my_labels = my_labels.batch(BATCH_SIZE)\n",
        "\n",
        "train_dataset_pinn = tf.data.Dataset.from_tensor_slices(dataPINN);\n",
        "train_labels = tf.data.Dataset.from_tensor_slices(labels);\n",
        "\n",
        "train_dataset_pinn = train_dataset_pinn.shuffle(BUFFER_SIZE, seed = RANDOM_SEED)\n",
        "train_dataset_pinn = train_dataset_pinn.batch(BATCH_SIZE)\n",
        "train_labels = train_labels.shuffle(BUFFER_SIZE, seed = RANDOM_SEED)\n",
        "train_labels = train_labels.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH7oUE16APAv"
      },
      "source": [
        "## Модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdqXuJUReXAj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.constraints import min_max_norm\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"\"\"95.7\t5.5\t2.22\n",
        "95.7\t4.7\t1.3\n",
        "95.7\t4.7\t1.3\n",
        "94.1-95.7\t10.4\t0.49\n",
        "94.4\t14.75\t0.07\n",
        "94.1-95.7\t10.4\t0.49\n",
        "94.4\t14.75\t0.07\n",
        "94.1-95.7\t10.4\t0.195\n",
        "5.9-7.0\t8.2-9.1\t0.235\n",
        "5.5\t7.35\t0.135\n",
        "5.5\t7.35\t0.135\n",
        "5.1-6.5\t8.8-10.4\t0.49-0.58\n",
        "5.1-6.5\t8.8-10.4\t0.49-0.58\n",
        "5.1\t3.4\t1.79\n",
        "5.1\t3.4\t1.79\n",
        "5.1\t3.8\t3.16 50 10\"\"\" # TODO может их перевернуть типо 2.22 5.5 95.7\n",
        "constraints = s.split()\n",
        "for i in range(len(constraints)):\n",
        "  if len(str(constraints[i]).split('-'))<=1:\n",
        "    constraints[i] = float(constraints[i])\n",
        "  else:\n",
        "    constraints[i] =sum([float(m) for m in constraints[i].split('-')])/2\n",
        "print(np.reshape(constraints, (50)))"
      ],
      "metadata": {
        "id": "u4KH1R7BQU-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con = np.array(np.reshape(constraints, (50)));\n",
        "print(con)"
      ],
      "metadata": {
        "id": "Zf4nwuBsQWdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaZk2z0o8_Nl"
      },
      "outputs": [],
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    model.add(Dense(27, use_bias=False, input_shape=(100,)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Dense(16*3, use_bias=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Dense(150*15*3, use_bias=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    model.add(Reshape((150, 15, 3)))\n",
        "    assert model.output_shape == (None, 150, 15, 3) # Note: None is the batch size\n",
        "    \n",
        "    model.add(Conv2DTranspose(3, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 150, 15, 3)\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    model.add(Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 300, 30, 3)\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    model.add(Conv2DTranspose(2, (5, 5), strides=(5, 5), padding='same', use_bias=False))    \n",
        "    assert model.output_shape == (None, 1500, 150, 2)\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))    \n",
        "    assert model.output_shape == (None, 3000, 300, 1)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kql20sikK58E"
      },
      "outputs": [],
      "source": [
        "noise_input = keras.Input(shape=(100,), name=\"noise\")\n",
        "\n",
        "dense_uf = layers.Flatten()(noise_input)\n",
        "dense_uf = layers.Dense(512, use_bias=True)(dense_uf)\n",
        "#dense = layers.BatchNormalization()(dense)\n",
        "dense_uf = layers.LeakyReLU()(dense_uf)\n",
        "\n",
        "dense = layers.Flatten()(dense_uf)\n",
        "dense = layers.Dense(1024, use_bias=True)(dense)\n",
        "#dense = layers.BatchNormalization()(dense)\n",
        "dense = layers.LeakyReLU()(dense)\n",
        "\n",
        "dense_af = layers.Flatten()(dense)\n",
        "dense_af = layers.Dense(256, use_bias=True)(dense_af)\n",
        "#dense = layers.BatchNormalization()(dense)\n",
        "dense_af = layers.LeakyReLU()(dense_af)\n",
        "\n",
        "param = layers.Flatten()(dense_af)\n",
        "param = layers.Dense(50, use_bias=True, bias_constraint=min_max_norm(min_value=2, max_value=100, rate=0.05))(param)\n",
        "#param = layers.BatchNormalization()(param)\n",
        "param = layers.Activation(activation='relu', name='Parameters_of_CV')(param) #layers.LeakyReLU(name='Parameters_of_CV')(param)\n",
        "\n",
        "dense2 = layers.Dense(150*15*3, use_bias=True)(param)\n",
        "dense2 = layers.BatchNormalization()(dense2)\n",
        "dense2 = layers.LeakyReLU()(dense2)\n",
        "\n",
        "dense2 = layers.Reshape((150, 15, 3))(dense2)\n",
        "\n",
        "conv1 = layers.Conv2DTranspose(3, (5, 5), strides=(1, 1), padding='same', use_bias=False)(dense2)\n",
        "conv1 = layers.BatchNormalization()(conv1)\n",
        "conv1 = layers.LeakyReLU()(conv1)\n",
        "\n",
        "conv2 = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False)(conv1)\n",
        "conv2 = layers.BatchNormalization()(conv2)\n",
        "conv2 = layers.LeakyReLU()(conv2)\n",
        "\n",
        "conv3 = layers.Conv2DTranspose(2, (5, 5), strides=(5, 5), padding='same', use_bias=False)(conv2)\n",
        "conv3 = layers.BatchNormalization()(conv3)\n",
        "conv3 = layers.LeakyReLU()(conv3)\n",
        "\n",
        "image_pred = layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh', name='Images')(conv3)\n",
        "#conv4 = layers.BatchNormalization()(conv4)\n",
        "#conv4 = layers.LeakyReLU()(conv4)\n",
        "\n",
        "generator = keras.Model(\n",
        "    inputs=[noise_input],\n",
        "    outputs=[param, image_pred],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K.set_value(generator.layers[11].weights[1], con)"
      ],
      "metadata": {
        "id": "oUXBTWDsaztB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.layers[11].get_weights()"
      ],
      "metadata": {
        "id": "LcPzMrWQQoCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyLHJu-L8_Nn"
      },
      "outputs": [],
      "source": [
        "#generator = make_generator_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQb0cGAoOAQR"
      },
      "outputs": [],
      "source": [
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRXFXRsVOdLK"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(generator, \"generator_model.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQhWNjHz8_No"
      },
      "outputs": [],
      "source": [
        "# Create a random noise and generate a sample\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)[1]\n",
        "# Visualize the generated sample\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dArYYO7V8_Nq"
      },
      "outputs": [],
      "source": [
        "generated_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77fbV1R88_Nq"
      },
      "outputs": [],
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same', input_shape=[3000, 300, 1]))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(1, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(27))\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(48))\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10))\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl6TKAuSQwZX"
      },
      "outputs": [],
      "source": [
        "image_input = keras.Input(shape=(3000, 300, 1), name=\"MRI\")\n",
        "\n",
        "conv_disc_1 = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(image_input)\n",
        "conv_disc_1 = layers.LeakyReLU()(conv_disc_1)\n",
        "conv_disc_1 = layers.Dropout(0.3)(conv_disc_1)\n",
        "\n",
        "conv_disc_2 = layers.Conv2D(1, (5, 5), strides=(2, 2), padding='same')(conv_disc_1)\n",
        "conv_disc_2 = layers.LeakyReLU()(conv_disc_2)\n",
        "conv_disc_2 = layers.Dropout(0.3)(conv_disc_2)\n",
        "\n",
        "\n",
        "dense_disc_1 = layers.Flatten()(conv_disc_2)\n",
        "dense_disc_1 = layers.Dense(27)(dense_disc_1)\n",
        "#param_disc = layers.BatchNormalization()(dense)\n",
        "dense_disc_1 = layers.LeakyReLU()(dense_disc_1)\n",
        "\n",
        "param_disc = layers.Flatten()(dense_disc_1)\n",
        "param_disc = layers.Dense(50)(param_disc)\n",
        "param_disc = layers.Activation(activation='relu', name='Parameters__Of_CV')(param_disc) #layers.LeakyReLU(name='Parameters__Of_CV')(param_disc) #sigmoid\n",
        "\n",
        "dense_disc_2 = layers.Flatten()(param_disc)\n",
        "dense_disc_2 = layers.Dense(10)(dense_disc_2)\n",
        "dense_disc_2 = layers.LeakyReLU()(dense_disc_2)\n",
        "\n",
        "diagnosis_output = layers.Flatten()(dense_disc_2)\n",
        "diagnosis_output = layers.Dense(1)(diagnosis_output)\n",
        "diagnosis_output = layers.LeakyReLU(name='Diagnosis')(diagnosis_output)\n",
        "\n",
        "\n",
        "discriminator = keras.Model(\n",
        "    inputs=[image_input],\n",
        "    outputs=[param_disc, diagnosis_output],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7lKueRETfdj"
      },
      "outputs": [],
      "source": [
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ECwkiBu8_Nt"
      },
      "outputs": [],
      "source": [
        "decision = discriminator(generated_image)\n",
        "print(decision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLD3cZMHVQHw"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(discriminator, \"discriminator_model.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-h9Ea9PS6Yl"
      },
      "outputs": [],
      "source": [
        "S1 = [[1, 2, 0],\n",
        " [3, 4, 1],\n",
        " [5, 6, 2],\n",
        " [4, 6, 7],\n",
        " [9, 10, 8],\n",
        " [10, 11, 13],\n",
        " [12, 9, 14],\n",
        " [14, 13, 15]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVoHqDrYS8Oi"
      },
      "outputs": [],
      "source": [
        "S2 = [[0, 15], [12, 3], [11, 5], [7, 8]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDJa_HBA-bpc"
      },
      "outputs": [],
      "source": [
        "s = \"\"\"95.7\t5.5\t2.22\n",
        "95.7\t4.7\t1.3\n",
        "95.7\t4.7\t1.3\n",
        "94.1-95.7\t10.4\t0.49\n",
        "94.4\t14.75\t0.07\n",
        "94.1-95.7\t10.4\t0.49\n",
        "94.4\t14.75\t0.07\n",
        "94.1-95.7\t10.4\t0.195\n",
        "5.9-7.0\t8.2-9.1\t0.235\n",
        "5.5\t7.35\t0.135\n",
        "5.5\t7.35\t0.135\n",
        "5.1-6.5\t8.8-10.4\t0.49-0.58\n",
        "5.1-6.5\t8.8-10.4\t0.49-0.58\n",
        "5.1\t3.4\t1.79\n",
        "5.1\t3.4\t1.79\n",
        "5.1\t3.8\t3.16\"\"\"\n",
        "constraints = s.split()\n",
        "for i in range(len(constraints)):\n",
        "  if len(str(constraints[i]).split('-'))<=1:\n",
        "    constraints[i] = float(constraints[i])\n",
        "  else:\n",
        "    constraints[i] =sum([float(m) for m in constraints[i].split('-')])/2\n",
        "print(np.reshape(constraints, (16,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW7gIEhzM6hF"
      },
      "source": [
        "0 = 15\n",
        "1 + 2 = 0\n",
        "3 + 4 = 1\n",
        "5 + 6 = 2\n",
        "12 = 3\n",
        "4 + 6 = 7\n",
        "11 = 5\n",
        "7 = 8\n",
        "9 + 10 = 8\n",
        "10 + 11 = 13\n",
        "12 + 9 = 14\n",
        "14 + 13 = 15\n",
        "\n",
        "0 = 1\n",
        "0 = 2\n",
        "0 = 15\n",
        "1 = 3\n",
        "1 = 4\n",
        "2 = 5 \n",
        "2 = 6\n",
        "7 = 4\n",
        "7 = 6\n",
        "7 = 8\n",
        "5 = 11\n",
        "13 = 10\n",
        "13 = 11\n",
        "13 = 15\n",
        "14 = 15\n",
        "14 = 12\n",
        "14 = 9\n",
        "8 = 9\n",
        "8 = 10\n",
        "12 = 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inHar8bC8_Nu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.gen_math_ops import sigmoid\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "\n",
        "def loss_CV(real_output, fake_output):\n",
        "  total_loss = [0]*real_output.shape[0]\n",
        "  for j in range(real_output.shape[0]):\n",
        "    total_loss[j] = [0]*(len(S1)*3+len(S2)*2)\n",
        "    #print('SHAPE:', len(total_loss), real_output.shape, real_output.shape[0])\n",
        "    for i in range(len(S2)):\n",
        "      #print(i, total_loss, S2[i], real_output)\n",
        "      A1,u1,p1 = real_output[j][S2[i][0]*3], real_output[j][S2[i][0]*3+1], real_output[j][S2[i][0]*3+2]\n",
        "      A2,u2,p2 = real_output[j][S2[i][1]*3], real_output[j][S2[i][1]*3+1], real_output[j][S2[i][1]*3+2]\n",
        "      total_loss[j][i*2] = A1*u1 - A2*u2\n",
        "      if i == 0: # Heart\n",
        "        total_loss[j][i*2+1] = 1.0*(p1 + 0.5*0.105*u1**2 + 0.001) / (p2 + real_output[j][-2] + 0.5*0.105*u2**2 + 0.001) - 1\n",
        "        #total_loss[j][i*2+1] = p1 - p2 - real_output[j][-2] + 0.5*0.105*(u1**2-u2**2)\n",
        "      else:      # Cap\n",
        "        total_loss[j][i*2+1] = 1.0*(p1 + 0.5*0.105*u1**2 + 0.001) / (p2 + real_output[j][-1] + 0.5*0.105*u2**2 + 0.001) - 1\n",
        "        #total_loss[j][i*2+1] = p1 - p2 - real_output[j][-1] + 0.5*0.105*(u1**2-u2**2)\n",
        "    \n",
        "    for i in range(len(S1)):\n",
        "      A1,u1,p1 = real_output[j][S1[i][0]*3], real_output[j][S1[i][0]*3+1], real_output[j][S1[i][0]*3+2]\n",
        "      A2,u2,p2 = real_output[j][S1[i][1]*3], real_output[j][S1[i][1]*3+1], real_output[j][S1[i][1]*3+2]\n",
        "      A3,u3,p3 = real_output[j][S1[i][2]*3], real_output[j][S1[i][2]*3+1], real_output[j][S1[i][2]*3+2]\n",
        "      total_loss[j][8+i*3] =  (1.0*A3*u3 + 0.001) / (A2*u2 + A1*u1 + 0.001) - 1 #A3*u3 - A2*u2 - A1*u1 #\n",
        "      total_loss[j][8+i*3+1] = 1.0*(p3 + 0.5*0.105*u3**2 + 0.001) / (p1 + 0.5*0.105*u1**2 + 0.001) - 1 #p3 - p1 + 0.5*0.105*(u3**2 -u1**2)  #\n",
        "      total_loss[j][8+i*3+2] = 1.0*(p3 + 0.5*0.105*u3**2 + 0.001) / (p2 + 0.5*0.105*u2**2 + 0.001) - 1#p3 - p2 + 0.5*0.105*(u3**2 -u2**2)#\n",
        "\n",
        "    #print(total_loss)\n",
        "  answer = mse(total_loss, fake_output) \n",
        "  #for i in range(real_output.shape[0]):\n",
        "  #for i in range(real_output.shape[0]):\n",
        "  #  last = 1\n",
        "  #  while last*3<real_output.shape[1]:\n",
        "  #    answer+=change(real_output[i][last*3-3],real_output[i][last*3-2],real_output[i][last*3-1],last)\n",
        "  #    last+=1\n",
        "  #print(answer)\n",
        "  return answer\n",
        "\n",
        "def interval_loss(real_output, fake_output):\n",
        "  #total_loss = [0]*real_output.shape[0]\n",
        "  \n",
        "  answer = 0\n",
        "  for j in range(real_output.shape[0]):\n",
        "    for i in range(16):\n",
        "      answer+=change(real_output[j][i*3-3],real_output[j][i*3-2],real_output[j][i*3-1],2)\n",
        "  #print('bruh', answer)\n",
        "  return answer\n",
        "\n",
        "def change(a,u,p,k,label=0):\n",
        "  c1 = constraints[k*3-3]\n",
        "  c2 = constraints[k*3-2]\n",
        "  c3 = constraints[k*3-1]\n",
        "  res_change = 0.0\n",
        "  if label==0:\n",
        "    if abs(p-c1)>c1*0.2:\n",
        "      res_change += abs(p-c1)-c1*0.2 \n",
        "    if abs(u-c2)>c2*0.2:\n",
        "      res_change += abs(u-c2)-c2*0.2\n",
        "    if abs(a-c3)>c3*0.2:\n",
        "      res_change += abs(a-c3)-c3*0.2\n",
        "  else:\n",
        "    if abs(p-c1)>c1*0.5:\n",
        "      res_change += abs(p-c1)-c1*0.5 \n",
        "    if abs(u-c2)>c2*0.5:\n",
        "      res_change += abs(u-c2)-c2*0.5\n",
        "    if abs(a-c3)>c3*0.5:\n",
        "      res_change += abs(a-c3)-c3*0.5\n",
        "  return res_change\n",
        "\n",
        "\n",
        "def discriminator_loss(real_output, fake_output, y_tr = None):\n",
        "    #print(real_output.numpy(), fake_output.numpy())\n",
        "    real_loss = cross_entropy(y_tr, real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    \n",
        "    #print(total_loss)\n",
        "\n",
        "    return total_loss #+ nev\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output) #+ nev\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab9cn2OI8_Nx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "#checkpoint_dir = 'gdrive/MyDrive/Colab_Notebooks/Images/Pinn_F/training_checkpoints'\n",
        "checkpoint_dir = DIR+'training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXCU1PogARgF"
      },
      "source": [
        "## Обучаем"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSBTI258Nadq"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5\n",
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "accuracy_cv, f1_cv, precision_cv, recall_cv, g_cv, d_cv, g_cv_cv, d_cv_cv = [], [], [], [], [], [], [], []\n",
        "num_examples_to_generate = 3 \n",
        "noise_dim = 100\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HMaL9Y-Nads"
      },
      "outputs": [],
      "source": [
        "# tf.function annotation causes the function \n",
        "# to be \"compiled\" as part of the training\n",
        "@tf.function\n",
        "def train_step(images, label = None):\n",
        "    # 1 - Create a random noise to feed it into the model\n",
        "    # for the image generation\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    \n",
        "    # 2 - Generate images and calculate loss values\n",
        "    # GradientTape method records operations for automatic differentiation.\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images[1], training=True)\n",
        "\n",
        "      gen_cv_loss = loss_CV(generated_images[0], tf.zeros((3,32)))\n",
        "      disc_cv_loss = loss_CV(real_output[0], tf.zeros((3,32))) + loss_CV(fake_output[0], tf.zeros((3,32)))\n",
        "\n",
        "      gen_loss = generator_loss(fake_output[1])\n",
        "      disc_loss = discriminator_loss(real_output[1], fake_output[1], label)\n",
        "\n",
        "      gen_interval_loss = interval_loss(generated_images[0], tf.zeros((3,50)))\n",
        "      disc_interval_loss = (interval_loss(real_output[0], tf.zeros((3,50))) + interval_loss(fake_output[0], tf.zeros((3,50))))/2\n",
        "\n",
        "      #print(\"fake_output\", fake_output[0], \"real_output\", real_output[0], \"images\" ,generated_images[0], \"gen_interval_loss\", gen_interval_loss, \"disc_interval_loss\", disc_interval_loss, sep='\\n')\n",
        "    #print(gen_tape_CV.gradient(gen_loss, generator.trainable_variables[1]))\n",
        "    \n",
        "    #print(gen_tape.gen_loss, generator.trainable_variables[1])\n",
        "\n",
        "    #print('GEN_TAPE: ', gen_tape)\n",
        "    #print('DISC_TAPE: ', disc_tape)\n",
        "\n",
        "\n",
        "    # 3 - Calculate gradients using loss values and model variables\n",
        "    # \"gradient\" method computes the gradient using \n",
        "    # operations recorded in context of this tape (gen_tape and disc_tape).\n",
        "    \n",
        "    # It accepts a target (e.g., gen_loss) variable and \n",
        "    # a source variable (e.g.,generator.trainable_variables)\n",
        "    # target --> a list or nested structure of Tensors or Variables to be differentiated.\n",
        "    # source --> a list or nested structure of Tensors or Variables.\n",
        "    # target will be differentiated against elements in sources.\n",
        "\n",
        "    # \"gradient\" method returns a list or nested structure of Tensors  \n",
        "    # (or IndexedSlices, or None), one for each element in sources. \n",
        "    # Returned structure is the same as the structure of sources.\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient([gen_loss, gen_cv_loss, gen_interval_loss], \n",
        "                                               generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient([disc_loss, disc_cv_loss, disc_interval_loss], \n",
        "                                                discriminator.trainable_variables)\n",
        "    \n",
        "    #gradients_of_generator_CV = gen_tape_CV.gradient(gen_cv_loss, \n",
        "    #                                           generator.trainable_variables)\n",
        "    #gradients_of_discriminator_CV = disc_tape_CV.gradient(disc_cv_loss, \n",
        "    #                                            discriminator.trainable_variables)     \n",
        "\n",
        "    # 4 - Process  Gradients and Run the Optimizer\n",
        "    # \"apply_gradients\" method processes aggregated gradients. \n",
        "    # ex: optimizer.apply_gradients(zip(grads, vars))\n",
        "    \"\"\"\n",
        "    Example use of apply_gradients:\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n",
        "    # Processing aggregated gradients.\n",
        "    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\n",
        "    \"\"\"\n",
        "\n",
        "    #generator_optimizer.apply_gradients(zip(gradients_of_generator_CV, generator.trainable_variables))\n",
        "    #discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator_CV, discriminator.trainable_variables))\n",
        "\n",
        "    \n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9fIGwU2Nadw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython import display # A command shell for interactive computing in Python.\n",
        "\n",
        "def train(dataset, label, epochs, current):\n",
        "  # A. For each epoch, do the following:\n",
        "  for epoch in range(current, epochs):\n",
        "    start = time.time()\n",
        "    # 1 - For each batch of the epoch, \n",
        "\n",
        "    i = 0\n",
        "    for image_batch, label_batch in zip(dataset, label):\n",
        "      \n",
        "      # 1.a - run the custom \"train_step\" function\n",
        "      # we just declared above\n",
        "      #print(my_label_batch)\n",
        "\n",
        "\n",
        "      train_step(image_batch, label_batch)\n",
        "      i+=1\n",
        "      print(\"Batch\",i,\"in dataset. Done\", time.time()-start)\n",
        "      print(generator.layers[11].get_weights()[1])\n",
        "\n",
        "    # 2 - Produce images for the GIF as we go\n",
        "    ###########################################display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "    \n",
        "    print(\"Image saved. Done\", time.time()-start)\n",
        "\n",
        "    # 3 - Save the model every 5 epochs as \n",
        "    # a checkpoint, which we will use later\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "      print(\"Checkpoint saved. Done\", time.time()-start)\n",
        "\n",
        "    # 4 - Save result of metrics\n",
        "    noise = tf.random.normal([10, noise_dim])\n",
        "    generated_images = generator(noise, training=False)\n",
        "    \n",
        "    print('Im okay')\n",
        "    \n",
        "    real_output = discriminator(test_dataPINN, training=False)\n",
        "    fake_output = discriminator(generated_images[1], training=False)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output[1])\n",
        "    disc_loss = discriminator_loss(real_output[1], fake_output[1], test_labels)\n",
        "\n",
        "    gen_cv_loss = loss_CV(generated_images[0], tf.zeros((generated_images[0].shape[0],32)))\n",
        "    disc_cv_loss = loss_CV(real_output[0], tf.zeros((real_output[0].shape[0],32))) + loss_CV(fake_output[0], tf.zeros((fake_output[0].shape[0],32)))\n",
        "\n",
        "    #print(generated_images[0], real_output[0], fake_output[0])\n",
        "    #print('Hey')\n",
        "    gen_interval_loss = interval_loss(generated_images[0], tf.zeros((3,32)))\n",
        "    #print('Hey2')\n",
        "    disc_interval_loss = (interval_loss(real_output[0], tf.zeros((3,32))) + interval_loss(fake_output[0], tf.zeros((3,32))))/2\n",
        "    #print('Hey3')\n",
        "\n",
        "    g_cv.append(gen_loss.numpy())\n",
        "    d_cv.append(disc_loss.numpy())\n",
        "\n",
        "    g_cv_cv.append(gen_cv_loss.numpy())\n",
        "    d_cv_cv.append(disc_cv_loss.numpy())\n",
        "\n",
        "    print(\"Metrics evaulated. Gen_Loss: \", gen_loss.numpy(), \"Gen_CV: \", gen_cv_loss.numpy(), \"Gen_interval\", gen_interval_loss.numpy(), \" Disc_loss: \", disc_loss.numpy(), \"Disc_CV: \", disc_cv_loss.numpy(), \"Disc_interval\", disc_interval_loss.numpy(), \". Done\", time.time()-start)\n",
        "    #f1, precision, recall = f1_score(y_val, y_val_pred_cat), precision_score(y_val, y_val_pred_cat), recall_score(y_val, y_val_pred_cat)\n",
        "    \n",
        "    y_tr = np.append(test_labels, np.zeros_like(fake_output[1].numpy())).reshape(31,1)\n",
        "    y_pr = np.append([0 if i <= 0 else 1 for i in real_output[1].numpy()], [0 if i <= 0 else 1 for i in fake_output[1].numpy()]).reshape(31,1)\n",
        "    #print(\"True: \", list(y_tr), \"\\nPred: \", list(y_pr))\n",
        "    #print('Real: ', real_output.numpy(), '\\nPred: ', fake_output.numpy())\n",
        "    print(accuracy_score(y_true=y_tr, y_pred=y_pr), end=' ')\n",
        "    print(precision_score(y_tr, y_pr),end=' ')\n",
        "    print(recall_score(y_tr, y_pr),end=' ')\n",
        "    print(f1_score(y_tr, y_pr))\n",
        "\n",
        "    accuracy_cv.append(accuracy_score(y_true=y_tr, y_pred=y_pr))\n",
        "    precision_cv.append(precision_score(y_tr, y_pr))\n",
        "    recall_cv.append(recall_score(y_tr, y_pr))\n",
        "    f1_cv.append(f1_score(y_tr, y_pr))\n",
        "\n",
        "    with open(DIR+\"test.txt\", \"a\") as file_object:\n",
        "      file_object.writelines(\"Ep: \" + str(epoch+1) + \" \" + str(accuracy_cv[-1]) + \" \" + str(f1_cv[-1]) + \" \" + str(precision_cv[-1]) + \" \" + str(recall_cv[-1]) + \" \" + str(g_cv[-1]) + \" \" + str(d_cv[-1]) + \" \" + str(g_cv_cv[-1])+ \" \" + str(d_cv_cv[-1]) + \" \" + str(gen_interval_loss) + \" \" + str(disc_interval_loss) + \"\\n\")\n",
        "\n",
        "    with open(DIR+'g_cv.txt', 'a') as file_object:\n",
        "      file_object.writelines(\"Ep: \" + str(epoch+1) + \" \" + str(g_cv[-1]) + \"\\n\")\n",
        "\n",
        "    with open(DIR+'d_cv.txt', 'a') as file_object:\n",
        "      file_object.writelines(\"Ep: \" + str(epoch+1) + \" \" + str(d_cv[-1]) + \"\\n\")\n",
        "\n",
        "    #np.savetxt('gdrive/MyDrive/Colab_Notebooks/Images/test.txt', (accuracy_cv,f1_cv,precision_cv,recall_cv,g_cv,d_cv))\n",
        "    #np.savetxt('gdrive/MyDrive/Colab_Notebooks/Images/g_cv.txt', g_cv)\n",
        "    #np.savetxt('gdrive/MyDrive/Colab_Notebooks/Images/d_cv.txt', d_cv)\n",
        "\n",
        "    # 5 - Print out the completed epoch no. and the time spent\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # B. Generate a final image after the training is completed\n",
        "  # display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aK3dAPpNad0"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  # 1 - Generate images\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  with open(DIR+\"Params.txt\", \"a\") as file_object:\n",
        "    file_object.writelines(\"Ep: \" + str(epoch+1) + \" \" + str(predictions[0].numpy()[0]) + \"\\n\")\n",
        "\n",
        "  # 2 - Plot the generated images\n",
        "  fig = plt.figure(figsize=(18,15))\n",
        "  l = 0\n",
        "  cm = 'gray'\n",
        "  for i in range(predictions[1].shape[0]):\n",
        "    arr = predictions[1][i, :, :, 0].reshape(10, 300, 300);\n",
        "    for j in range(10):\n",
        "      l = l + 1\n",
        "      plt.subplot(6,5,l)\n",
        "      plt.imshow(arr[j], cmap=cm)\n",
        "      plt.axis('off')\n",
        "    #cm = 'binary'\n",
        "    #plt.subplot(4, 4, i+1)\n",
        "    #plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "    #plt.axis('off')\n",
        "  # 3 - Save the generated images\n",
        "  plt.savefig(DIR+'image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  \n",
        "  #plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFbIfGWiATL9"
      },
      "source": [
        "## Train(5,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_ENsU9Sbv6l"
      },
      "outputs": [],
      "source": [
        "#tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB7M-4SMIg7L"
      },
      "outputs": [],
      "source": [
        "train(train_dataset_pinn, train_labels, 10, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6UD6HhUhT4Q"
      },
      "outputs": [],
      "source": [
        "train(train_dataset_pinn, train_labels, 50, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGFssMe5jG2h"
      },
      "outputs": [],
      "source": [
        "train(train_dataset_pinn, train_labels, 100, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IOuofDJwC74"
      },
      "outputs": [],
      "source": [
        "train(train_dataset_pinn, train_labels, 150, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBY4rqsXwEvN"
      },
      "outputs": [],
      "source": [
        "train(train_dataset_pinn, train_labels, 200, 150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twv7ENfJwGe9"
      },
      "outputs": [],
      "source": [
        "train(train_dataset_pinn, train_labels, 250, 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Qd_eB8nwIda"
      },
      "outputs": [],
      "source": [
        "train(train_dataset_pinn, train_labels, 300, 250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwuIJMO5wJyf"
      },
      "outputs": [],
      "source": [
        "train(train_dataset_pinn, train_labels, 350, 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d-UyTkgwLqH"
      },
      "outputs": [],
      "source": [
        "train(train_dataset_pinn, train_labels, 400, 350)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6KH3DkhwNXc"
      },
      "outputs": [],
      "source": [
        "train(train_dataset_pinn, train_labels, 450, 400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS-HTYU4wPiq"
      },
      "outputs": [],
      "source": [
        "train(train_dataset_pinn, train_labels, 500, 450)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1Wqhv0gEyF6"
      },
      "outputs": [],
      "source": [
        "# Create a random noise and generate a sample\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "print(generated_image[0].numpy()[0])\n",
        "# Visualize the generated sample\n",
        "plt.imshow(generated_image[1][0, :, :, 0], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXWJMRFJGSfY"
      },
      "outputs": [],
      "source": [
        "decision = discriminator(generated_image[1], training = False)\n",
        "print(decision[0])\n",
        "print(decision[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5dr6708Gs6h"
      },
      "outputs": [],
      "source": [
        "decision = discriminator(test_dataPINN[1].reshape(1, 3000, 300, 1), training = False)\n",
        "print(decision[0])\n",
        "print(decision[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ILqNdTKHZbb"
      },
      "outputs": [],
      "source": [
        "decision = discriminator(test_dataPINN[0].reshape(1, 3000, 300, 1), training = False)\n",
        "print(decision[0])\n",
        "print(decision[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHMBR2zfHU5x"
      },
      "outputs": [],
      "source": [
        "test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMhtuVDEQ_8N"
      },
      "outputs": [],
      "source": [
        "noise = tf.random.normal([21, noise_dim])\n",
        "\n",
        "generated_images = generator(noise, training=False)\n",
        "\n",
        "real_output = discriminator(test_dataPINN, training=False)\n",
        "fake_output = discriminator(generated_images, training=False)\n",
        "\n",
        "gen_loss = generator_loss(fake_output)\n",
        "disc_loss = discriminator_loss(real_output, fake_output, test_labels, test_dataPINN)\n",
        "\n",
        "print(gen_loss, disc_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GqHNsc9Q_8Q"
      },
      "outputs": [],
      "source": [
        "fake_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqSY-v_5Q_8R"
      },
      "outputs": [],
      "source": [
        "real_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_V0b6c5Q_8S"
      },
      "outputs": [],
      "source": [
        "test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObMMhbWTAVhV"
      },
      "source": [
        "## Восстановление чекпоинта"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw_BzfNhRaUq"
      },
      "outputs": [],
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rscrsp5MAYe3"
      },
      "source": [
        "## Результаты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCw5TWBeRd1w"
      },
      "outputs": [],
      "source": [
        "# PIL is a library which may open different image file formats\n",
        "import PIL \n",
        "# Display a single image using the epoch number\n",
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open(DIR+'image_at_epoch_{:04d}.png'.format(epoch_no))\n",
        "display_image(EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4faUEQxRd1y"
      },
      "outputs": [],
      "source": [
        "import glob # The glob module is used for Unix style pathname pattern expansion.\n",
        "import imageio # The library that provides an easy interface to read and write a wide range of image data\n",
        "\n",
        "anim_file = DIR+'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob(DIR+'image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  i = 0\n",
        "  for filename in filenames:\n",
        "    i+=1\n",
        "    print(filename, i)\n",
        "    image = imageio.imread(filename)\n",
        "    if i>200:\n",
        "      writer.append_data(image)\n",
        "  # image = imageio.imread(filename)\n",
        "  # writer.append_data(image)\n",
        "  \n",
        "display.Image(open(DIR+'dcgan.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukMVHECWAaqm"
      },
      "source": [
        "### Графики"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OflQN4sdRp6Z"
      },
      "outputs": [],
      "source": [
        "ac, f1, pr, re, d, g, d_cv, g_cv, d_plus_cv, g_plus_cv = [],[],[],[],[],[],[],[],[],[]\n",
        "\n",
        "with open('gdrive/MyDrive/Colab_Notebooks/Images/GAN_with_Pinn_rules/test.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        lol = line.split()\n",
        "        ac.append(float(lol[2]))\n",
        "        f1.append(float(lol[3]))\n",
        "        pr.append(float(lol[4]))\n",
        "        re.append(float(lol[5]))\n",
        "        g.append(float(lol[6]))\n",
        "        d.append(float(lol[7]))\n",
        "\n",
        "        g_cv.append(float(lol[8]))\n",
        "        d_cv.append(float(lol[9]))\n",
        "\n",
        "        g_plus_cv.append(float(lol[6])+float(lol[8]))\n",
        "        d_plus_cv.append(float(lol[7])+float(lol[9]))\n",
        "\n",
        "\n",
        "        #accuracy_.append(float(lol[3]))\n",
        "        #f1_.append(float(lol[4]))\n",
        "        #precision_.append(float(lol[5]))\n",
        "        #recall_.append(float(lol[6]))\n",
        "        #d_cv_for_graph.append(float(line.split()[2]))\n",
        "print(ac[-43:], f1[-43:], pr[-43:], re[-43:], d[-43:], g[-43:], d_cv[-43:], g_cv[-43:], d_plus_cv[-43:], g_plus_cv[-43:], sep='\\n')\n",
        "print(len(ac[-43:]), len(d[-43:]), len(d_cv[-43:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoHZtF5QRp6c"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d)+1)\n",
        "y1 = ac\n",
        "y2 = f1\n",
        "y3 = pr\n",
        "y4 = re\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(x, y1, '-r', label=\"acc\", lw=5, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, '-g', label=\"f1\", mec='g', lw=4, mew=2, ms=10)\n",
        "plt.plot(x, y3, '-b', label=\"prec\", lw=3, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y4, '-y', label=\"rec\", mec='y', lw=2, mew=2, ms=10)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vgGcNOvac3Q"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d)+1)\n",
        "y1 = d\n",
        "y2 = g\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.title('Loss (BinaryCrossentropy)')\n",
        "plt.plot(x, y1, '-r', label=\"discr_loss\", lw=5, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, '-g', label=\"gener_loss\", mec='r', lw=2, mew=2, ms=12)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysFqC4tGafN3"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d)+1)\n",
        "y1 = d_cv\n",
        "y2 = g_cv\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.title('Residuals (PINN)')\n",
        "plt.plot(x, y1, '-r', label=\"discr_loss\", lw=5, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, '-g', label=\"gener_loss\", mec='r', lw=2, mew=2, ms=12)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzV5v-RtbJoI"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d[-43:])+1)\n",
        "y1 = d_plus_cv[-43:]\n",
        "y2 = g_plus_cv[-43:]\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.title('Loss (Combined)')\n",
        "plt.plot(x, y1, 'o-r', label=\"discr_loss\", lw=5, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, 'v-g', label=\"gener_loss\", mec='r', lw=2, mew=2, ms=12)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcwT_uiYiJ6n"
      },
      "outputs": [],
      "source": [
        "ac, f1, pr, re, d, g, d_cv, g_cv, d_plus_cv, g_plus_cv = [],[],[],[],[],[],[],[],[],[]\n",
        "\n",
        "with open('gdrive/MyDrive/Colab_Notebooks/Images/Pre_Pinn_F/test.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        lol = line.split()\n",
        "        ac.append(float(lol[2]))\n",
        "        f1.append(float(lol[3]))\n",
        "        pr.append(float(lol[4]))\n",
        "        re.append(float(lol[5]))\n",
        "        g.append(float(lol[6]))\n",
        "        d.append(float(lol[7]))\n",
        "\n",
        "        g_cv.append(float(lol[8]))\n",
        "        d_cv.append(float(lol[9]))\n",
        "\n",
        "        g_plus_cv.append(float(lol[6])+float(lol[8]))\n",
        "        d_plus_cv.append(float(lol[7])+float(lol[9]))\n",
        "\n",
        "\n",
        "        #accuracy_.append(float(lol[3]))\n",
        "        #f1_.append(float(lol[4]))\n",
        "        #precision_.append(float(lol[5]))\n",
        "        #recall_.append(float(lol[6]))\n",
        "        #d_cv_for_graph.append(float(line.split()[2]))\n",
        "print(ac[20:], f1[20:], pr[20:], re[20:], d[20:], g[20:], d_cv[20:], g_cv[20:], d_plus_cv[20:], g_plus_cv[20:], sep='\\n')\n",
        "print(len(ac[20:]), len(d), len(d_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHP0aSOeiz43"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d[20:])+1)\n",
        "y1 = ac[20:]\n",
        "y2 = f1[20:]\n",
        "y3 = pr[20:]\n",
        "y4 = re[20:]\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(x, y1, '-r', label=\"acc\", lw=5, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, '-g', label=\"f1\", mec='g', lw=4, mew=2, ms=10)\n",
        "plt.plot(x, y3, '-b', label=\"prec\", lw=3, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y4, '-y', label=\"rec\", mec='y', lw=2, mew=2, ms=10)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THTWaHNPi2mW"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d[20:])+1)\n",
        "y1 = d[20:]\n",
        "y2 = g[20:]\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.title('Loss (BinaryCrossentropy)')\n",
        "plt.plot(x, y1, 'o-r', label=\"discr_loss\", lw=5, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, 'v-g', label=\"gener_loss\", mec='r', lw=2, mew=2, ms=12)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T79ADcmi6ie"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d[20:])+1)\n",
        "y1 = d_cv[20:]\n",
        "y2 = g_cv[20:]\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.title('Residuals (PINN)')\n",
        "plt.plot(x, y1, 'o-r', label=\"discr_loss\", lw=5, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, 'v-g', label=\"gener_loss\", mec='r', lw=2, mew=2, ms=12)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrmOKKLui9wG"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d[20:])+1)\n",
        "y1 = d_plus_cv[20:]\n",
        "y2 = g_plus_cv[20:]\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.title('Loss (Combined)')\n",
        "plt.plot(x, y1, 'o-r', label=\"discr_loss\", lw=5, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, 'v-g', label=\"gener_loss\", mec='r', lw=2, mew=2, ms=12)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehsag6lVlU01"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d_cv_for_graph)+1)\n",
        "y1 = accuracy_\n",
        "y2 = f1_\n",
        "y3 = precision_\n",
        "y4 = recall_\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(x, y1, '-r', label=\"acc\", lw=5, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, '-g', label=\"f1\", mec='g', lw=2, mew=2, ms=10)\n",
        "plt.plot(x, y3, '-b', label=\"prec\", lw=2, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y4, '-y', label=\"rec\", mec='y', lw=2, mew=2, ms=10)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCVJ-0iPlWdF"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d_cv_for_graph)+1)\n",
        "y1 = d_cv_for_graph\n",
        "y2 = g_cv_for_graph\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(x, y1, '-r', label=\"discr_loss\", lw=2, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, '-g', label=\"gener_loss\", mec='r', lw=2, mew=2, ms=12)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhCnHk4zlYky"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d_cv_for_graph)+1)\n",
        "y1 = np.log(d_cv_for_graph)\n",
        "y2 = np.log(g_cv_for_graph)\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(x, y1, '-r', label=\"discr_loss\", lw=2, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, '-g', label=\"gener_loss\", mec='r', lw=2, mew=2, ms=12)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBbRwLKiRp6d"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d_cv_for_graph)+1)\n",
        "y1 = accuracy_\n",
        "y2 = f1_\n",
        "y3 = precision_\n",
        "y4 = recall_\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(x, y1, '-r', label=\"acc\", lw=5, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, '-g', label=\"f1\", mec='g', lw=2, mew=2, ms=10)\n",
        "plt.plot(x, y3, '-b', label=\"prec\", lw=2, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y4, '-y', label=\"rec\", mec='y', lw=2, mew=2, ms=10)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXwAoKGsRp6e"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d_cv_for_graph)+1)\n",
        "y1 = d_cv_for_graph\n",
        "y2 = g_cv_for_graph\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(x, y1, '-r', label=\"discr_loss\", lw=2, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, '-g', label=\"gener_loss\", mec='r', lw=2, mew=2, ms=12)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMVPQ45FRp6f"
      },
      "outputs": [],
      "source": [
        "x = range(1, len(d_cv_for_graph)+1)\n",
        "y1 = np.log(d_cv_for_graph)\n",
        "y2 = np.log(g_cv_for_graph)\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(x, y1, '-r', label=\"discr_loss\", lw=2, mec='b', mew=2, ms=10)\n",
        "plt.plot(x, y2, '-g', label=\"gener_loss\", mec='r', lw=2, mew=2, ms=12)\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D3oPB5kRp6g"
      },
      "outputs": [],
      "source": [
        "#Inception Score \n",
        "#Frechet Inception Distance"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "k3PYlaRtrrQx",
        "1Rr9J5cHAIFh"
      ],
      "name": "GAN_with_PINN_with_constraint_Bias.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}